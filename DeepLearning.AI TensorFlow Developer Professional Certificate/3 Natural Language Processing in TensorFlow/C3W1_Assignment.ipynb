{"cells":[{"cell_type":"markdown","id":"juvenile-enforcement","metadata":{"id":"juvenile-enforcement"},"source":["# Week 1: Explore the BBC News archive\n","\n","Welcome! In this assignment you will be working with a variation of the [BBC News Classification Dataset](https://www.kaggle.com/c/learn-ai-bbc/overview), which contains 2225 examples of news articles with their respective categories (labels).\n","\n","Let's get started!"]},{"cell_type":"code","execution_count":4,"id":"combined-brooklyn","metadata":{"id":"combined-brooklyn","tags":["graded"],"executionInfo":{"status":"ok","timestamp":1657218633096,"user_tz":-330,"elapsed":3,"user":{"displayName":"Apurva Sinha","userId":"16475837108159642150"}}},"outputs":[],"source":["import csv\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"markdown","id":"dependent-power","metadata":{"id":"dependent-power"},"source":["Begin by looking at the structure of the csv that contains the data:"]},{"cell_type":"code","execution_count":5,"id":"finite-panic","metadata":{"tags":["graded"],"colab":{"base_uri":"https://localhost:8080/","height":201},"id":"finite-panic","executionInfo":{"status":"error","timestamp":1657218638723,"user_tz":-330,"elapsed":10,"user":{"displayName":"Apurva Sinha","userId":"16475837108159642150"}},"outputId":"c9475672-60dd-4172-9a74-68d1e4746f90"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-84cfbf6b9b51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./bbc-text.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"First line (header) looks like this:\\n\\n{csvfile.readline()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Each data point looks like this:\\n\\n{csvfile.readline()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './bbc-text.csv'"]}],"source":["with open(\"./bbc-text.csv\", 'r') as csvfile:\n","    print(f\"First line (header) looks like this:\\n\\n{csvfile.readline()}\")\n","    print(f\"Each data point looks like this:\\n\\n{csvfile.readline()}\")     "]},{"cell_type":"markdown","id":"aggregate-calvin","metadata":{"id":"aggregate-calvin"},"source":["As you can see, each data point is composed of the category of the news article followed by a comma and then the actual text of the article."]},{"cell_type":"markdown","id":"rocky-credit","metadata":{"id":"rocky-credit"},"source":["## Removing Stopwords\n","\n","One important step when working with text data is to remove the **stopwords** from it. These are the most common words in the language and they rarely provide useful information for the classification process.\n","\n","Complete the `remove_stopwords` below. This function should receive a string and return another string that excludes all of the stopwords provided."]},{"cell_type":"code","execution_count":null,"id":"permanent-privilege","metadata":{"tags":["graded"],"id":"permanent-privilege"},"outputs":[],"source":["# GRADED FUNCTION: remove_stopwords\n","def remove_stopwords(sentence):\n","    \"\"\"\n","    Removes a list of stopwords\n","    \n","    Args:\n","        sentence (string): sentence to remove the stopwords from\n","    \n","    Returns:\n","        sentence (string): lowercase sentence without the stopwords\n","    \"\"\"\n","    # List of stopwords\n","    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n","    \n","    # Sentence converted to lowercase-only\n","    sentence = sentence.lower()\n","    sentence = ' '.join([item for item in sentence.split(' ') if item not in stopwords])\n","    return sentence"]},{"cell_type":"code","execution_count":null,"id":"northern-third","metadata":{"tags":["graded"],"id":"northern-third"},"outputs":[],"source":["# Test your function\n","remove_stopwords(\"I am about to go to the store and get any snack\")"]},{"cell_type":"markdown","id":"pressed-excellence","metadata":{"id":"pressed-excellence"},"source":["***Expected Output:***\n","```\n","'go store get snack'\n","\n","```"]},{"cell_type":"markdown","id":"animal-photography","metadata":{"id":"animal-photography"},"source":["## Reading the raw data\n","\n","Now you need to read the data from the csv file. To do so, complete the `parse_data_from_file` function.\n","\n","A couple of things to note:\n","- You should omit the first line as it contains the headers and not data points.\n","- There is no need to save the data points as numpy arrays, regular lists is fine.\n","- To read from csv files use [`csv.reader`](https://docs.python.org/3/library/csv.html#csv.reader) by passing the appropriate arguments.\n","- `csv.reader` returns an iterable that returns each row in every iteration. So the label can be accessed via row[0] and the text via row[1].\n","- Use the `remove_stopwords` function in each sentence."]},{"cell_type":"code","execution_count":null,"id":"monthly-beginning","metadata":{"tags":["graded"],"id":"monthly-beginning"},"outputs":[],"source":["# GRADED FUNCTION: parse_data_from_file\n","def parse_data_from_file(filename):\n","    \"\"\"\n","    Extracts sentences and labels from a CSV file\n","    \n","    Args:\n","        filename (string): path to the CSV file\n","    \n","    Returns:\n","        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n","    \"\"\"\n","    sentences = []\n","    labels = []\n","    with open(filename, 'r') as csvfile:\n","        ### START CODE HERE\n","        reader = csv.reader(csvfile, delimiter=',')\n","        next(reader,None)\n","        for i in reader:\n","            labels.append(i[0])\n","            sentences.append(remove_stopwords(i[1]))\n","#         ### END CODE HERE\n","    return sentences, labels"]},{"cell_type":"code","execution_count":null,"id":"listed-saturn","metadata":{"tags":["graded"],"id":"listed-saturn"},"outputs":[],"source":["# Test your function\n","sentences, labels = parse_data_from_file(\"./bbc-text.csv\")\n","\n","print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n","print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n","print(f\"There are {len(labels)} labels in the dataset.\\n\")\n","print(f\"The first 5 labels are {labels[:5]}\")"]},{"cell_type":"markdown","id":"favorite-shanghai","metadata":{"id":"favorite-shanghai"},"source":["***Expected Output:***\n","```\n","There are 2225 sentences in the dataset.\n","\n","First sentence has 436 words (after removing stopwords).\n","\n","There are 2225 labels in the dataset.\n","\n","The first 5 labels are ['tech', 'business', 'sport', 'sport', 'entertainment']\n","\n","```"]},{"cell_type":"markdown","id":"diverse-basket","metadata":{"id":"diverse-basket"},"source":["## Using the Tokenizer\n","\n","Now it is time to tokenize the sentences of the dataset. \n","\n","Complete the `fit_tokenizer` below. \n","\n","This function should receive the list of sentences as input and return a [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) that has been fitted to those sentences. You should also define the \"Out of Vocabulary\" token as `<OOV>`."]},{"cell_type":"code","execution_count":null,"id":"cultural-virginia","metadata":{"tags":["graded"],"id":"cultural-virginia"},"outputs":[],"source":["# GRADED FUNCTION: fit_tokenizer\n","def fit_tokenizer(sentences):\n","    \"\"\"\n","    Instantiates the Tokenizer class\n","    \n","    Args:\n","        sentences (list): lower-cased sentences without stopwords\n","    \n","    Returns:\n","        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n","    \"\"\"\n","    ### START CODE HERE\n","    # Instantiate the Tokenizer class by passing in the oov_token argument\n","    tokenizer = Tokenizer(oov_token=\"<OOV>\")\n","    tokenizer.fit_on_texts(sentences)# Fit on the sentences\n","    \n","    ### END CODE HERE\n","    return tokenizer"]},{"cell_type":"code","execution_count":null,"id":"tracked-hostel","metadata":{"tags":["graded"],"id":"tracked-hostel"},"outputs":[],"source":["tokenizer = fit_tokenizer(sentences)\n","word_index = tokenizer.word_index\n","print(word_index)\n","print(f\"Vocabulary contains {len(word_index)} words\\n\")\n","print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"]},{"cell_type":"markdown","id":"moderate-pollution","metadata":{"id":"moderate-pollution"},"source":["***Expected Output:***\n","```\n","Vocabulary contains 29714 words\n","\n","<OOV> token included in vocabulary\n","\n","```"]},{"cell_type":"code","execution_count":null,"id":"golden-flash","metadata":{"tags":["graded"],"id":"golden-flash"},"outputs":[],"source":["# GRADED FUNCTION: get_padded_sequences\n","def get_padded_sequences(tokenizer, sentences):\n","    \"\"\"\n","    Generates an array of token sequences and pads them to the same length\n","    \n","    Args:\n","        tokenizer (object): Tokenizer instance containing the word-index dictionary\n","        sentences (list of string): list of sentences to tokenize and pad\n","    \n","    Returns:\n","        padded_sequences (array of int): tokenized sentences padded to the same length\n","    \"\"\"\n","    \n","    ### START CODE HERE\n","    # Convert sentences to sequences\n","    sequences = tokenizer.texts_to_sequences(sentences)\n","    \n","    # Pad the sequences using the post padding strategy\n","    padded_sequences = pad_sequences(sequences, padding='post')\n","    ### END CODE HERE\n","    \n","    return padded_sequences"]},{"cell_type":"code","execution_count":null,"id":"spanish-entrepreneur","metadata":{"tags":["graded"],"id":"spanish-entrepreneur","outputId":"25cb4ba3-1f37-4916-e5d7-81fc95a06deb"},"outputs":[{"name":"stdout","output_type":"stream","text":["First padded sequence looks like this: \n","\n","[  96  176 1157 ...    0    0    0]\n","\n","Numpy array of all sequences has shape: (2225, 2438)\n","\n","This means there are 2225 sequences in total and each one has a size of 2438\n"]}],"source":["padded_sequences = get_padded_sequences(tokenizer, sentences)\n","print(f\"First padded sequence looks like this: \\n\\n{padded_sequences[0]}\\n\")\n","print(f\"Numpy array of all sequences has shape: {padded_sequences.shape}\\n\")\n","print(f\"This means there are {padded_sequences.shape[0]} sequences in total and each one has a size of {padded_sequences.shape[1]}\")"]},{"cell_type":"markdown","id":"wired-brief","metadata":{"id":"wired-brief"},"source":["***Expected Output:***\n","```\n","First padded sequence looks like this: \n","\n","[  96  176 1157 ...    0    0    0]\n","\n","Numpy array of all sequences has shape: (2225, 2438)\n","\n","This means there are 2225 sequences in total and each one has a size of 2438\n","\n","```"]},{"cell_type":"code","execution_count":null,"id":"unknown-optimization","metadata":{"tags":["graded"],"id":"unknown-optimization"},"outputs":[],"source":["# GRADED FUNCTION: tokenize_labels\n","def tokenize_labels(labels):\n","    \"\"\"\n","    Tokenizes the labels\n","    \n","    Args:\n","        labels (list of string): labels to tokenize\n","    \n","    Returns:\n","        label_sequences, label_word_index (list of string, dictionary): tokenized labels and the word-index\n","    \"\"\"\n","    ### START CODE HERE\n","    \n","    # Instantiate the Tokenizer class\n","    # No need to pass additional arguments since you will be tokenizing the labels\n","    label_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n","    \n","    # Fit the tokenizer to the labels\n","    label_tokenizer.fit_on_texts(labels)\n","    \n","    # Save the word index\n","    label_word_index = label_tokenizer.word_index\n","    \n","    # Save the sequences\n","    label_sequences = label_tokenizer.texts_to_sequences(labels)\n","\n","    ### END CODE HERE\n","    \n","    return label_sequences, label_word_index"]},{"cell_type":"code","execution_count":null,"id":"streaming-conviction","metadata":{"tags":["graded"],"id":"streaming-conviction","outputId":"65355290-ead2-4000-fd63-a38106969232"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary of labels looks like this {'<OOV>': 1, 'sport': 2, 'business': 3, 'politics': 4, 'tech': 5, 'entertainment': 6}\n","\n","First ten sequences [[5], [3], [2], [2], [6], [4], [4], [2], [2], [6]]\n","\n"]}],"source":["label_sequences, label_word_index = tokenize_labels(labels)\n","print(f\"Vocabulary of labels looks like this {label_word_index}\\n\")\n","print(f\"First ten sequences {label_sequences[:10]}\\n\")"]},{"cell_type":"markdown","id":"endangered-poultry","metadata":{"id":"endangered-poultry"},"source":["***Expected Output:***\n","```\n","Vocabulary of labels looks like this {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n","\n","First ten sequences [[4], [2], [1], [1], [5], [3], [3], [1], [1], [5]]\n","\n","```"]},{"cell_type":"markdown","id":"cross-chess","metadata":{"id":"cross-chess"},"source":["**Congratulations on finishing this week's assignment!**\n","\n","You have successfully implemented functions to process various text data processing ranging from pre-processing, reading from raw files and tokenizing text.\n","\n","**Keep it up!**"]}],"metadata":{"dlai_version":"1.2.0","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"C3W1_Assignment.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}